---
title: 'MB-TaylorFormer: Multi-branch Efficient Transformer Expanded by Taylor Formula for Image Dehazing'
date: 2023-10-27
permalink: /posts/2023/10/27/MB-TaylorFormer Multi-branch Efficient Transformer Expanded by Taylor Formula for Image Dehazing/
tags:
  - DeepLearning
  - Transformer
  - Image Dehazing
---

- **论文链接：**[https://arxiv.org/abs/2308.14036](https://arxiv.org/abs/2308.14036)
- **代码链接：**[https://github.com/FVL2020/ICCV-2023-MB-TaylorFormer](https://github.com/FVL2020/ICCV-2023-MB-TaylorFormer)
- **会议：** ICCV(2023)
- **Keywords**： Transformer, Image Dehazing



# Abastract

在图像去雾任务中，受 softmax-attention 二次计算（quadratic computational）复杂度的限制，Transformer 架构没有得到广泛应用。为了解决这一问题，作者提出了一种新的 Transformer 架构，使用泰勒展开来逼近 softmax-attention，并实现了线形计算复杂度。





> 这里提到的二次计算复杂度和线性计算复杂度，是否可以理解为数据结构中的 $O(n^2)$ 和 $O(n)$



# 附

* https://zhuanlan.zhihu.com/p/658312750
